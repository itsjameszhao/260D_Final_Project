{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"NMt_0AnBBiOr"},"outputs":[],"source":["# toggle pathing\n","drive_path = '/content/drive/MyDrive/UCLA Fall 2024/[Shared] CS 260D Large Scale Machine Learning/Final Project'\n","# drive_path = '/content/drive/MyDrive/[Shared] CS 260D Large Scale Machine Learning/Final Project'\n","data_path = rf'{drive_path}/data'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yzn6fBRrBy88"},"outputs":[],"source":["#this allows reimported py source files to be reloaded\n","%load_ext autoreload\n","%autoreload 2\n","\n","# Import necessary libraries\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from torchsummary import summary\n","import matplotlib.pyplot as plt\n","import os\n","import sys\n","\n","# Optionally, mount Google Drive if you want to save the data to Google Drive for persistence\n","# Uncomment the following lines if you'd like to save to Google Drive\n","from google.colab import drive\n","# drive.mount('/content/drive')\n","# data_path = '/content/drive/My Drive/Colab Notebooks/data'\n","drive.mount('/content/drive', force_remount=True)\n","\n","\n","sys.path.append(drive_path) # Define the path in the Shared Drive\n","\n","# Otherwise, define a local path in the current Colab working directory\n","# data_path = './data'\n","\n","# Check for GPU support\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Ensure the local 'data' directory exists\n","os.makedirs(data_path, exist_ok=True)\n","\n","# Download the MNIST dataset to the specified data path\n","transform = transforms.Compose([transforms.ToTensor()])\n","train_dataset = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)\n","train_loader = DataLoader(train_dataset, batch_size=2**8, shuffle=True)"]},{"cell_type":"markdown","source":["### Find size of desired subset"],"metadata":{"id":"I9SI-77IWb9K"}},{"cell_type":"code","source":["############################################\n","# Warning, this takes a few minutes to run\n","############################################\n","# Import necessary libraries\n","import pickle\n","import gzip\n","import os\n","\n","train_dataset = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)\n","sample_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","\n","# unpack a bunch of examples\n","size_test_images = []\n","size_test_labels = []\n","for images, label_batch in sample_loader:\n","    for image in images:\n","      size_test_images.append(image)\n","    for label in label_batch:\n","      size_test_labels.append(label)\n","    # break\n","\n","k = 150 # starting point just for visualizing, this takes a while to run\n","serialized_size = 0\n","compressed_size = 0\n","x = []\n","y_comp = []\n","y_uncomp = []\n","\n","while compressed_size < 3: # size in MB\n","  sset = (size_test_labels[:k], size_test_images[:k]) # choose sset above\n","  # Serialize the data with gzip compression to reduce size\n","  serialized_data = pickle.dumps(sset)\n","  compressed_data = gzip.compress(serialized_data)\n","\n","  # Calculate the size of the compressed serialized data\n","  serialized_size = len(serialized_data) / (1024 * 1024)  # Convert bytes to MB\n","  compressed_size = len(compressed_data) / (1024 * 1024)  # Convert bytes to MB\n","  x.append(k)\n","\n","  y_uncomp.append(serialized_size)\n","  y_comp.append(compressed_size)# y_comp.append(compressed_size)\n","\n","  # Calculate total size of MNIST dataset on disk\n","  # def get_folder_size(path):\n","  #     total_size = 0\n","  #     for dirpath, dirnames, filenames in os.walk(path):\n","  #         for f in filenames:\n","  #             fp = os.path.join(dirpath, f)\n","  #             total_size += os.path.getsize(fp)\n","  #     return total_size / (1024 * 1024)  # Convert bytes to MB\n","\n","  # print(f\"Size of subset:\\t\\t k={k}, {serialized_size:.2f} MB\")\n","  # print(f\"Size of compressed sset: k={k}, {compressed_size:.2f} MB\")\n","  # print('===========')\n","  k +=10\n"],"metadata":{"id":"xX_h6kNcWXPw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy import stats\n","import numpy as np\n","# Now we have a linear regression for dataset size vs # of examples to extrapolate from\n","slope_comp, intercept_comp, r_comp, p_comp, std_err_comp = stats.linregress(x, y_comp)\n","plt.plot(x, y_comp, 'bo', label='compressed')\n","plt.plot(x, np.multiply(slope_comp,x) + intercept_comp, 'b-')\n","\n","# slope_uncomp, intercept_uncomp, r_uncomp, p_uncomp, std_err_uncomp = stats.linregress(x, y_uncomp)\n","# plt.plot(x, y_uncomp, 'ro', label='uncompressed')\n","# plt.plot(x, np.multiply(slope_uncomp,x) + intercept_uncomp, 'r-')\n","\n","target_size = 3 # MB\n","k_examples = round((target_size - intercept_comp)/slope_comp) # define subset size to extract\n","\n","plt.plot(k_examples, target_size, 'g*', markersize=10, label=f'target size = {target_size}MB')\n","\n","plt.xlabel('# of examples')\n","plt.ylabel('Size (MB)')\n","plt.legend()\n","plt.show()\n","\n"],"metadata":{"id":"MrIaDDgVWbEN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Random Subset Selection"],"metadata":{"id":"4voJRH4Jxsbu"}},{"cell_type":"code","source":["# select randomly from 10k points\n","train_dataset = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)\n","sample_loader = DataLoader(train_dataset, batch_size=10000, shuffle=True)\n","for batch, (images, labels) in enumerate(sample_loader):\n","  images = images.to(device)\n","  labels = labels.to(device)\n","  break\n","\n","indices = torch.randperm(len(images))[:k_examples]\n","sset_random = images[indices]\n","slabels_random = labels[indices]\n","\n","# Step 3: Create DataLoader for the reconstructed dataset\n","random_subset = ReconstructedMNISTDataset(sset_random, slabels_random)\n","\n","batch_size = 64  # Define your preferred batch size\n","sset_random_loader = DataLoader(random_subset, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"NQ5gNtdRxrqI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize and train the classifier on reconstructed images\n","classifier = RegularCNNClassifier().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(classifier.parameters(), lr=0.001)"],"metadata":{"id":"-ozpskdr1UNu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","# Enable interactive mode for plotting\n","plt.ion()\n","\n","# Create a figure and axes for the loss curve\n","fig, ax = plt.subplots()\n","loss_line, = ax.plot([], [], 'b-', label='Training Loss')  # Create an empty line for the plot\n","ax.set_xlabel('Batch')\n","ax.set_ylabel('Loss')\n","ax.set_title('Real-time Training Loss')\n","ax.legend()\n","\n","# Initialize list to store batch loss values for plotting\n","batch_losses = []\n","\n","# Training the classifier on reconstructed images\n","num_epochs = 200  # You can adjust this as needed\n","for epoch in (pbar := tqdm(range(num_epochs))):\n","    epoch_loss = 0.0  # Track epoch loss\n","    for batch_idx, (images, labels) in enumerate(sset_random_loader):\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = classifier(images)\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","        batch_losses.append(loss.item())  # Append batch loss\n","\n","        # Update plot with current batch loss\n","        loss_line.set_data(range(len(batch_losses)), batch_losses)\n","        ax.relim()  # Recompute the data limits\n","        ax.autoscale_view()  # Autoscale the view\n","        fig.canvas.draw()\n","        fig.canvas.flush_events()  # Update the plot in real-time\n","\n","    avg_epoch_loss = epoch_loss / len(sset_random_loader)\n","    # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_epoch_loss:.4f}')\n","    pbar.set_description(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_epoch_loss:.4f}')\n","\n","plt.ioff()  # Turn off interactive mode after training\n","plt.show()  # Keep the plot window open"],"metadata":{"id":"Pjz6zi66zQGV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the MNIST test dataset\n","test_dataset = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","# Evaluate the classifier on the real MNIST test set\n","classifier.eval()\n","correct = 0\n","total = 0\n","all_predictions = []\n","all_labels = []\n","\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = classifier(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","        all_predictions.extend(predicted.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","accuracy = 100 * correct / total\n","print(f'Test Accuracy of the model on the 10000 test images: {accuracy:.2f}%')\n","\n","\n","from sklearn.metrics import classification_report, confusion_matrix\n","import seaborn as sns\n","\n","# Generate classification report\n","print(classification_report(all_labels, all_predictions))\n","\n","# Generate confusion matrix\n","cm = confusion_matrix(all_labels, all_predictions)\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"True\")\n","plt.title(\"Confusion Matrix\")\n","plt.show()"],"metadata":{"id":"oXiYAt2a1KBo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Clustering by raw images (based on example)"],"metadata":{"id":"JbIlTjh7S_0w"}},{"cell_type":"code","source":["# Standard Libraries\n","import os\n","import numpy as np\n","import pandas as pd\n","\n","# Modeling and Machine Learning\n","from IPython.display import Image\n","from sklearn.manifold import TSNE\n","\n","from sklearn.decomposition import TruncatedSVD\n","########################################\n","# select images\n","train_loader = DataLoader(train_dataset, batch_size=10000, shuffle=True)\n","for batch, (images, labels) in enumerate(train_loader):\n","  images = images.to(device).cpu()\n","  labels = labels.to(device).cpu()\n","  break\n","########################################\n","tsvd = TruncatedSVD(n_components=50).fit_transform(images.reshape(-1, 28*28))\n","\n","# # Fit t-SNE on the Truncated SVD reduced data (50 features)\n","tsne = TSNE()\n","transformed = tsne.fit_transform(tsvd)\n","# # Split up the t-SNE results in training and testing data\n","tsne_train = pd.DataFrame(transformed, columns=['component1', 'component2'])\n","tsne_test = pd.DataFrame(transformed, columns=['component1', 'component2'])\n","\n","tsne_train['labels'] = labels"],"metadata":{"id":"yJL7BatsTioC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install scikit-learn-extra\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn_extra.cluster import KMedoids\n","## this is done on ALL of the data, doesn't really scale. need to split this up into smaller chunks\n","N = 10  # Number of classes (MNIST has 10 digits)\n","k = k_examples # subset size\n","\n","train_loader = DataLoader(train_dataset, batch_size=10000, shuffle=True)\n","kmeans = KMedoids(n_clusters=k, random_state=0, init='k-medoids++').fit(tsne_train)\n","medoids = kmeans.cluster_centers_\n","medoid_indices = kmeans.medoid_indices_\n","# display_sample_images(images[medoid_indices], labels[medoid_indices], k=5, N=int(len(medoid_indices)))"],"metadata":{"id":"HIGVNKD0TbBm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.graph_objects as go\n","import matplotlib.pyplot as plt\n","from pylab import *\n","colors = [matplotlib.colors.rgb2hex(c) for c in plt.cm.plasma(np.linspace(0,1,N))] # plotly hex colormap, N # of discrete colors\n","\n","set_labels = set(tsne_train['labels'])\n","\n","fig = go.Figure()\n","for label in set_labels:\n","  fig.add_trace(go.Scatter(x=tsne_train[tsne_train['labels']==label]['component1'],\n","                           y=tsne_train[tsne_train['labels']==label]['component2'],\n","                          #  line=dict(width=2,color=colors[int(label)]),\n","                           marker=dict(opacity=0.5,color=colors[int(label)]),\n","                           hovertext=f\"Label:{label}\",\n","                           mode='markers', name=label, legendgroup=int(label)))\n","for medoid in medoids:\n","  fig.add_trace(go.Scatter(x=[medoid[0]], y=[medoid[1]], mode='markers', name=int(medoid[2]),\n","                           hovertext=f\"{medoid[2]}\",\n","                           marker=dict(size=12, line=dict(width=2,color='black')),\n","                           line=dict(width=2,color=colors[int(medoid[2])]),\n","                           legendgroup=int(medoid[2])\n","                              ))\n","fig.update_layout(\n","    width=800, height=800,\n","    template='seaborn',\n","    xaxis = dict(title=\"Component 1\", range=[-40,40]),\n","    yaxis = dict(title=\"Component 2\", range=[-40,40]),\n","    title='Medoid Selection',\n","    legend=dict(groupclick=\"toggleitem\")\n",")\n","fig.show()"],"metadata":{"id":"nkgxcRvRS-sr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Step 1: Define a custom Dataset for the reconstructed images\n","class ReconstructedMNISTDataset(Dataset):\n","    def __init__(self, images, labels):\n","        \"\"\"\n","        Args:\n","            images (list of Tensors): List of reconstructed images as PyTorch Tensors.\n","            labels (list of int): List of corresponding labels for each image.\n","        \"\"\"\n","        self.images = images\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        # Get image and label by index\n","        image = self.images[idx]\n","        label = self.labels[idx]\n","        return image, label\n","\n","# Step 2: Initialize the custom dataset\n","subset_data = ReconstructedMNISTDataset(images[medoid_indices], labels[medoid_indices])\n","\n","# Step 3: Create DataLoader for the reconstructed dataset\n","batch_size = 64  # Define your preferred batch size\n","subset_loader = DataLoader(subset_data, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"9KWmNI_k6cNF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RegularCNNClassifier(nn.Module):\n","    def __init__(self):\n","        super(RegularCNNClassifier, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n","        self.fc1 = nn.Linear(in_features=16 * 2 * 2, out_features=32)\n","        self.fc2 = nn.Linear(in_features=32, out_features=16)\n","        self.fc3 = nn.Linear(in_features=16, out_features=10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = torch.max_pool2d(x, 2)\n","        x = x.view(-1, 16 * 2 * 2)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","# Initialize and train the classifier on reconstructed images\n","classifier = RegularCNNClassifier().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(classifier.parameters(), lr=0.001)"],"metadata":{"id":"Q6fH203_8XzI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","# Enable interactive mode for plotting\n","plt.ion()\n","\n","# Create a figure and axes for the loss curve\n","fig, ax = plt.subplots()\n","loss_line, = ax.plot([], [], 'b-', label='Training Loss')  # Create an empty line for the plot\n","ax.set_xlabel('Batch')\n","ax.set_ylabel('Loss')\n","ax.set_title('Real-time Training Loss')\n","ax.legend()\n","\n","# Initialize list to store batch loss values for plotting\n","batch_losses = []\n","\n","# Training the classifier on reconstructed images\n","num_epochs = 200  # You can adjust this as needed\n","for epoch in (pdb := tqdm(range(num_epochs))):\n","    epoch_loss = 0.0  # Track epoch loss\n","    for batch_idx, (images, labels) in enumerate(subset_loader):\n","        images, labels = images.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = classifier(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","        batch_losses.append(loss.item())  # Append batch loss\n","\n","        # Update plot with current batch loss\n","        loss_line.set_data(range(len(batch_losses)), batch_losses)\n","        ax.relim()  # Recompute the data limits\n","        ax.autoscale_view()  # Autoscale the view\n","        fig.canvas.draw()\n","        fig.canvas.flush_events()  # Update the plot in real-time\n","\n","    avg_epoch_loss = epoch_loss / len(subset_loader)\n","    # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_epoch_loss:.4f}')\n","    pdb.set_description(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_epoch_loss:.4f}')\n","\n","plt.ioff()  # Turn off interactive mode after training\n","plt.show()  # Keep the plot window open"],"metadata":{"id":"jsc0mid04eWq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the MNIST test dataset\n","test_dataset = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","# Evaluate the classifier on the real MNIST test set\n","classifier.eval()\n","correct = 0\n","total = 0\n","all_predictions = []\n","all_labels = []\n","\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = classifier(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","        all_predictions.extend(predicted.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","accuracy = 100 * correct / total\n","print(f'Test Accuracy of the model on the 10000 test images: {accuracy:.2f}%')\n","\n","\n","from sklearn.metrics import classification_report, confusion_matrix\n","import seaborn as sns\n","\n","# Generate classification report\n","print(classification_report(all_labels, all_predictions))\n","\n","# Generate confusion matrix\n","cm = confusion_matrix(all_labels, all_predictions)\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"True\")\n","plt.title(\"Confusion Matrix\")\n","plt.show()"],"metadata":{"id":"0Jjnsbq84X1m"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}